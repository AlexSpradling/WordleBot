{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Build a Pretty Good Wordle Bot\n",
    "\n",
    "Author: Alex Spradling\n",
    "\n",
    "---\n",
    "A lot of folks have done this, and probably done it better, but this is how I did it. This notebook will cover building a WORDLE bot algorithm from the ground up. The final WORDLE bot solves all WORDLE problems in around 3.5 guesses on average which is very close to the \"mathematically optimal\" solution. I'm happy enough with the results and happier to share them with you. So here we go, here's how you build a WORDLE bot that is pretty good. \n",
    "\n",
    "\n",
    "Note to Readers: I know these Jupyter Notebooks seem intimidating. The combination of word blocks, codeblocks, and formulae are weird -- but I promise I'll spend  the majority of my time writing about the mathematical and logical intuitions behind its function. The code that carries it out is just a tool, so if the presence of the code blocks puts you off, fear not. This isn't a CS class. \n",
    "\n",
    "---\n",
    "\n",
    "# WORDLE BOT! \n",
    "\n",
    "The heart of algorithm is based on INFORMATIONAL ENTROPY, which at its core is just some 9th grade math applied in a very clever way.\n",
    "\n",
    "INFORMATIONAL ENTROPY was developed by Claude Shannon at Bell Labs in 1948. It provides a quantitative measure of the uncertainty or randomness in a set of outcomes. Let's delve into the foundational concepts and the mathematical formulation of entropy.\n",
    "\n",
    "## Defining Key Concepts\n",
    "\n",
    "1. **Information**: In the context of information theory, 'information' quantifies the reduction in uncertainty. When an event occurs that we were uncertain about, we gain information. The more uncertain the event, the more information it provides.\n",
    "\n",
    "2. **Bit**: A 'bit' is the basic unit of information in information theory. It represents 1 unit of information in the form of a binary choice. A decision that can result in two equally likely outcomes provides one bit of information -- If I flip a fair coin and tell you I have Heads, you now know that the other side must be Tails -- we've eliminated 1 piece of uncertainty (and therefore gained 1 piece of information).\n",
    "\n",
    "3. **Probability**: The chance of a random event happening. A basic probability is found by dividing your preferred event by all other possible events. (The probability of getting heads when flipping a fair coin is .5 -- one event/two possible outcomes = 1/2 = .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Building Inuition \n",
    "## Information as Uncertainty\n",
    "\n",
    "If I flip a (fair) coin and tell you the side facing up is heads I've given you a little bit (to be precise, one bit) of information. \n",
    "\n",
    "You know, as a matter of fact, that a coin has two sides, heads and tails, and that it turns out that flipping a coin many times results in one side or the other coming up in my palm with equal probability. \n",
    "\n",
    "By telling you that Heads is up *you also know that tails is down* -- you've gained that 1 piece of information. \n",
    "\n",
    "Information Entropy rigorously reconciles this concept of uncertainty and information. \n",
    "\n",
    "\n",
    "## The Formula for Entropy\n",
    "\n",
    "$$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $$\n",
    "\n",
    "- **The Summation $\\sum$**: Represents the sum of information from all possible outcomes.\n",
    "- **Probability $P(x_i)$**: The likelihood of each outcome.\n",
    "- **Logarithm $\\log_2$**: A logarithm, in simple terms, tells us how many times we need to multiply a base number (like 2) by itself to get another number. For example, $\\log_2 8 = 3$ because $2 \\times 2 \\times 2 = 8$. \n",
    "- **Why Base 2?**: In information theory, we often deal with scenarios that have two outcomes, like flipping a coin. Using a base-2 logarithm is natural here because it directly relates to these binary (two-option) scenarios. Each bit of information represents a choice between two equally likely possibilities. Thus, the base-2 logarithm measures the number of binary choices (or bits) needed to encode the information from an outcome.\n",
    "\n",
    "For a fair coin (H for Heads, T for Tails), each with a probability of 0.5:\n",
    "\n",
    "$$ H(X) = -[P(H) \\log_2 P(H) + P(T) \\log_2 P(T)] $$\n",
    "$$ H(X) = -[0.5 \\log_2 0.5 + 0.5 \\log_2 0.5] $$\n",
    "$$ H(X) = -[-0.5 - 0.5] $$\n",
    "$$ H(X) = 1 \\text{ bit} $$\n",
    "\n",
    "So, we see that as defined above, 1 piece of information is an event with a binary outcome, I tell you I've got Heads, and since you know that we have an equal probability of getting Tails you can eliminate the uncertainty that the coin could be showing tails. That's the information you just gained. \n",
    "\n",
    "## Stay With Me!\n",
    "\n",
    "Now, let's consider a six-sided die. Unlike the coin toss, a dice roll has more outcomes, each with its own probability. If I roll a fair dice and tell you that I rolled a 1, I've informed you of more than just the outcome. Implicitly, I've also told you that the dice *isn't showing 2, 3, 4, 5, or 6.*\n",
    "\n",
    "This means you've gained more information from the result of a dice roll than from a coin flip because more potential outcomes are eliminated based on the lower probability of each outcome occurring. \n",
    "\n",
    "The Math:\n",
    "\n",
    "\n",
    "Applying our entropy formula:\n",
    "\n",
    "$$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $$\n",
    "\n",
    "For a fair dice, our random variable $X$ can take six values (1, 2, 3, 4, 5, 6), each with a probability of 1/6. The calculation is:\n",
    "\n",
    "$$ H(X) = -\\sum_{i=1}^{6} \\frac{1}{6} \\log_2 \\frac{1}{6} $$\n",
    "$$ H(X) = -6 \\times \\left( \\frac{1}{6} \\log_2 \\frac{1}{6} \\right) $$\n",
    "$$ H(X) = -\\log_2 \\frac{1}{6} $$\n",
    "$$ H(X) \\approx 2.58 \\text{ bits} $$\n",
    "\n",
    "\n",
    "Ok, so hopefully things are starting to make a little bit more sense now. We know a 1 on a fair die also means NOT 2,3,4,5,6, which is more information than just 1 is not 2. But as we look at the calculation above and see 2.58 bits as the result and the definition of a bit is \"1 piece of information\" which is further defined as a result that can be narrowed down to 2 equally likely outcomes we start to get confused again -- what exactly is 2.58 decisions that result in equally likely outcomes?\n",
    "\n",
    "\n",
    "\n",
    "Consider you're trying to guess the outcome of a die roll:\n",
    "\n",
    "In this scenario, the path to the answer may vary, sometimes being shorter or longer, which is why the average information content is a fractional number like 2.58. This represents the average amount of uncertainty resolved, or information gained, from observing the outcome of a fair die roll.\n",
    "    \n",
    "1. **First Question: \"Is the number greater than 3?\"**\n",
    "   - If Yes: The possible outcomes are 4, 5, or 6.\n",
    "   - If No: The possible outcomes are 1, 2, or 3.\n",
    "\n",
    "2. **Second Question:**\n",
    "   - If the answer to the first question was Yes:\n",
    "     - Ask: \"Is the number less than 5?\"\n",
    "       - If Yes: The outcome is 4.\n",
    "       - If No: The outcomes are 5 or 6.\n",
    "   - If the answer to the first question was No:\n",
    "     - Ask: \"Is the number greater than 1?\"\n",
    "       - If Yes: The outcomes are 2 or 3.\n",
    "       - If No: The outcome is 1.\n",
    "\n",
    "3. **Third Question:**\n",
    "   - Only needed if there are still two possibilities after the second question.\n",
    "     - If the remaining possibilities are 5 and 6:\n",
    "       - Ask: \"Is the number less than 6?\"\n",
    "         - If Yes: The outcome is 5.\n",
    "         - If No: The outcome is 6.\n",
    "     - If the remaining possibilities are 2 and 3:\n",
    "       - Ask: \"Is the number less than 3?\"\n",
    "         - If Yes: The outcome is 2.\n",
    "         - If No: The outcome is 3.\n",
    "\n",
    "In this scenario, the path to the answer may vary, sometimes being shorter or longer, which is why the average information content is a fractional number like 2.58. This represents the average amount of uncertainty resolved, or information gained, from observing the outcome of a fair die roll.\n",
    "\n",
    "### More Certainty = Less Uncertainty to Eliminate = Less Information\n",
    "\n",
    "Ok, you understand more uncertainty = more information. What does less uncertainty = less information look like?\n",
    "\n",
    "Back to the coins. What if the coin was *not* fair, it's loaded and it will come up heads more often than tails. We start flipping the coin and it's coming up heads more than tails and this isn't really surprising to you because *you know* that I've skewed the probabilities and the fact that heads is coming up more than tails just isn't really news to you. So with less uncertainty, we gain a little less information. \n",
    "\n",
    "Let's do the math again, we'll say that heads comes up 70% of the time with my new fixed coin.\n",
    "\n",
    "$$ H(X) = -[P_1 \\log_2 P_1 + P_2 \\log_2 P_2] $$\n",
    "$$ H(X) = -[0.7 \\log_2 0.7 + 0.3 \\log_2 0.3] $$\n",
    "$$ H(X) = -[0.7 \\times -0.5146 + 0.3 \\times -1.7370] $$\n",
    "$$ H(X) = -[-0.3602 - 0.5211] $$\n",
    "$$ H(X) \\approx 0.8813 \\text{ bits} $$\n",
    "\n",
    "I've rigged the results and you are less surprised and therefore less informed by the results of the coin flip, we can see that the calculated information is now $.88$ bits where it was once $1$ when we had a fair coin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practical Application\n",
    "\n",
    "So now that we know that higher entropy = more uncertainty eliminated, and we know how to calculate entropy, we can turn to a practical example, WORDLE. \n",
    "\n",
    "\n",
    "Our goal here is to select words that eliminate the most uncertainty, and narrow down the population of possible answer choices as best we can. \n",
    "\n",
    "We'll start first with a list of words. WORDLE uses a curated list of 5 letter words that is 2316 words long, but for now it'll be a lot simpler to explain if we just use a much smaller list:\n",
    "\n",
    "word_pool = `[\n",
    "    \"apple\", \"arise\", \"angle\", \"amble\", \"acorn\", \"alien\", \"align\", \"ankle\", \"agent\", \"adore\",\n",
    "    \"baker\", \"blaze\", \"brine\", \"brace\", \"broil\", \"beach\", \"brain\", \"brief\", \"brave\", \"bread\",\n",
    "    \"chair\", \"chase\", \"child\", \"choke\", \"climb\", \"clock\", \"cloud\", \"clown\", \"crown\", \"crisp\",\n",
    "    \"dance\", \"drape\", \"drive\", \"drown\", \"drake\", \"dread\", \"dream\", \"dress\", \"drill\", \"drink\",\n",
    "    \"eagle\", \"earth\", \"elbow\", \"elder\", \"elect\", \"email\", \"enter\", \"event\", \"every\", \"excel\",\n",
    "    \"fairy\", \"faith\", \"false\", \"fancy\", \"feast\", \"field\", \"fiery\", \"fight\", \"final\", \"flame\",\n",
    "    \"grace\", \"grade\", \"grain\", \"grand\", \"grant\", \"grape\", \"grass\", \"great\", \"green\", \"greet\"\n",
    "]`\n",
    "\n",
    "We'll also need a secret_word = `\"flame\"`. This is the word we're trying to guess.\n",
    "\n",
    "Ok, the game is set and the algo is on! \n",
    "\n",
    "Our goal is to algorithmically find the word that best divides the list using Entropy. \n",
    "\n",
    "Step 1 is to figure out a feedback system like the one uses in wordle. For our purposes we'll use this sytem: We'll traverse each letter of our guess and compare it to the secret word, if the letter is a match, it's a `+`, if the letter is in the `secret_word` but not in the correct position, we'll use a `-` and if the letter isn't in the `secret_word` we'll use a `0`. The function I'll code below called `get_feedback_pattern` does this work for us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# path to word_lists\n",
    "word_lists_path = os.path.join(os.getcwd(), 'word_lists')\n",
    "\n",
    "from word_lists.officialguesses import official_guesses\n",
    "from word_lists.officialanswers import official_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback_pattern(guess, answer):\n",
    "    used_indices = set()  # Track used indices in the answer\n",
    "    pattern = ['0'] * len(guess)  # Initialize feedback pattern with '0's\n",
    "\n",
    "    # First pass: Check for correct letters in the correct position\n",
    "    for i, (g, a) in enumerate(zip(guess, answer)):\n",
    "        if g == a:\n",
    "            pattern[i] = '+'\n",
    "            used_indices.add(i)\n",
    "\n",
    "    # Second pass: Check for correct letters in the wrong position\n",
    "    for i, g in enumerate(guess):\n",
    "        if pattern[i] == '0' and g in answer:\n",
    "            # Ensure the letter is not already matched\n",
    "            for j, a in enumerate(answer):\n",
    "                if g == a and j not in used_indices:\n",
    "                    pattern[i] = '-'\n",
    "                    used_indices.add(j)\n",
    "                    break\n",
    "\n",
    "    return tuple(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll use it below on an example guess, the word `great`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '0', '-', '-', '0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_feedback_pattern(guess = 'great', answer = 'flame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And above we see the coded feedback string `0+--0` for the guess `great` (remember we're trying to guess the word `flame`).\n",
    "\n",
    "Next, we'll compare our guess to every list in our `word_pool` and get a dictionary of patterns and a count of their occurences in our `word_pool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_patterns: 31\n",
      "Top 5 most common patterns by count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('0', '0', '-', '-', '0'), 11),\n",
       " (('0', '+', '0', '0', '0'), 6),\n",
       " (('0', '+', '-', '-', '0'), 5),\n",
       " (('0', '0', '0', '0', '0'), 5),\n",
       " (('0', '-', '0', '-', '0'), 3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pool = [\n",
    "    \"apple\", \"arise\", \"angle\", \"amble\", \"acorn\", \"alien\", \"align\", \"ankle\", \"agent\", \"adore\",\n",
    "    \"baker\", \"blaze\", \"brine\", \"brace\", \"broil\", \"beach\", \"brain\", \"brief\", \"brave\", \"bread\",\n",
    "    \"chair\", \"chase\", \"child\", \"choke\", \"climb\", \"clock\", \"cloud\", \"clown\", \"crown\", \"crisp\",\n",
    "    \"dance\", \"drape\", \"drive\", \"drown\", \"drake\", \"dread\", \"dream\", \"dress\", \"drill\", \"drink\",\n",
    "    \"eagle\", \"earth\", \"elbow\", \"elder\", \"elect\", \"email\", \"enter\", \"event\", \"every\", \"excel\",\n",
    "    \"fairy\", \"faith\", \"false\", \"fancy\", \"feast\", \"field\", \"fiery\", \"fight\", \"final\", \"flame\",\n",
    "    \"grace\", \"grade\", \"grain\", \"grand\", \"grant\", \"grape\", \"grass\", \"great\", \"green\", \"greet\"\n",
    "]\n",
    "\n",
    "\n",
    "guess = 'great'\n",
    "\n",
    "pattern_counts = {}\n",
    "for answer in word_pool:\n",
    "    pattern = get_feedback_pattern(guess, answer)\n",
    "    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1\n",
    "\n",
    "# get first 5 most common patterns\n",
    "\n",
    "print('total_patterns:', len(pattern_counts))\n",
    "print('Top 5 most common patterns by count:')\n",
    "sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the first 5 rows of big table of patterns, the results show that as we compare the word `great` to our `word_pool` the pattern `('0', '0', '-', '-', '0')` occured 11 times, the pattern `('0', '+', '0', '0', '0')` occured 6 times and so on...In total the word `great` yielded 31 different feedback patterns against the `word_pool`. \n",
    "\n",
    "---\n",
    "### Let's stop here and plant a quick flag. \n",
    "\n",
    "You might be tempted to think that a word that produces the most feedback patterns is the most effective word. This is not the case. \n",
    "\n",
    "- A word that generates the most feedback patterns may have a large number of unique patterns, but this doesn't necessarily mean it has the highest entropy.\n",
    "\n",
    "- If most of these patterns are highly unlikely (i.e., they have low probability), and only a few patterns are significantly more likely, the overall entropy might not be maximized. This is because entropy considers both the number and the probability distribution of these patterns.\n",
    "\n",
    "- For example, a guess that could equally likely be completely wrong, partially correct, or entirely correct (in terms of letter positions and presence) would have high entropy. In contrast, a guess that is almost always entirely wrong or right, with little variation, would have lower entropy despite possibly generating many feedback patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### Ok, let's keep marching towards calculating the Entropy of `great`\n",
    "\n",
    "We'll want to convert these 31 patterns into probabilities. Remember at the beginning we stated \"A basic probability is found by dividing your preferred event by all other possible events.\" \n",
    "\n",
    "Taking 1 of the 31 patterns, `('0', '0', '-', '-', '0')` as our pattern, let's compute the probability of occurence. We know it occured 11 times in the `word_pool`, so the probability of this pattern occuring in the word pool is 11/ all patterns, or 70, giving us a probability of .1571. We do this for all 31 of the yielded patterns. Let's use the function defined below to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common patterns and their probabilities:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('0', '0', '-', '-', '0'), 0.15714285714285714),\n",
       " (('0', '+', '0', '0', '0'), 0.08571428571428572),\n",
       " (('0', '+', '-', '-', '0'), 0.07142857142857142),\n",
       " (('0', '0', '0', '0', '0'), 0.07142857142857142),\n",
       " (('0', '-', '0', '-', '0'), 0.04285714285714286)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_patterns = len(word_pool)\n",
    "\n",
    "def calculate_pattern_probabilities(pattern_counts, total_patterns):\n",
    "    probabilities = {}\n",
    "    for pattern, count in pattern_counts.items():\n",
    "        probabilities[pattern] = count / total_patterns\n",
    "    return probabilities\n",
    "\n",
    "# get probabilities of 5 most common patterns\n",
    "\n",
    "pattern_probabilities = calculate_pattern_probabilities(pattern_counts, total_patterns)\n",
    "\n",
    "\n",
    "print('Top 5 most common patterns and their probabilities:')\n",
    "sorted(pattern_probabilities.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of 31 probabilities for patterns, generated against the `word_pool`, using the word `great`. Phew. Almost there. \n",
    "\n",
    "Let's revisit the formula for entropy and get our bearings straight:\n",
    "\n",
    "$$ H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i) $$\n",
    "\n",
    "We've got all of our probabilities for our patterns, now its a matter of multiplying each probability by $\\log_2 P(x_i)$ and summing everything up. The function below will do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(pattern_probabilities):\n",
    "    entropy = 0\n",
    "    for probability in pattern_probabilities.values():\n",
    "        entropy -= probability * math.log(probability, 2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the word \"great\": 4.510538320213657\n"
     ]
    }
   ],
   "source": [
    "entropy =  calculate_entropy(pattern_probabilities)\n",
    "print('Entropy of the word \"great\":', entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So now the big question is, was `great` a great (ha) word to use? \n",
    "\n",
    "Is it effectively eliminating the most uncertainty? \n",
    "\n",
    "Another way of thinking of this is, is the word generating the most diverse set of feedback patterns? \n",
    "\n",
    "We'll need some results to know. Let's see how many of the original pool of 70 words are eliminated by the feedback patterns generated by `great`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_guess_list(guess_list, current_guess, feedback):\n",
    "    refined_list = []\n",
    "\n",
    "    for word in guess_list:\n",
    "        remaining_letters = list(word)  # Track remaining letters\n",
    "        match = True\n",
    "\n",
    "        # First Pass: Correctly Placed Letters\n",
    "        for i, letter in enumerate(current_guess):\n",
    "            if feedback[i] == '+':\n",
    "                if word[i] != letter:\n",
    "                    match = False\n",
    "                    break\n",
    "                remaining_letters.remove(letter)  # Remove matched letter\n",
    "\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        # Second Pass: Misplaced Letters\n",
    "        for i, letter in enumerate(current_guess):\n",
    "            if feedback[i] == '-':\n",
    "                if letter not in remaining_letters or letter == word[i]:\n",
    "                    match = False\n",
    "                    break\n",
    "                remaining_letters.remove(letter)  # Remove matched letter\n",
    "            elif feedback[i] == '0' and letter in remaining_letters:\n",
    "                match = False\n",
    "                break\n",
    "\n",
    "        if match:\n",
    "            refined_list.append(word)\n",
    "\n",
    "    return refined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pool of Words: 70\n",
      "Words remaining after using \"great\" as a guess: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words remaining</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ankle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words remaining\n",
       "0            apple\n",
       "1            amble\n",
       "2            alien\n",
       "3            ankle\n",
       "4            blaze\n",
       "5            beach\n",
       "6            chase\n",
       "7            dance\n",
       "8            email\n",
       "9            false\n",
       "10           flame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Starting Pool of Words:', len(word_pool))\n",
    "\n",
    "words_remaining = refine_guess_list(word_pool, 'great', get_feedback_pattern(guess = 'great', answer = 'flame'))\n",
    "\n",
    "print('Words remaining after using \"great\" as a guess:', len(words_remaining))\n",
    "\n",
    "# df of words remaining\n",
    "words_remaining_df = pd.DataFrame(words_remaining, columns=['words remaining'])\n",
    "words_remaining_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `great` seems like a pretty good word in this case. We've eliminated all but 11 of the 70 words in the pool and are well on our way to solving the `secret_word` (`flame`).\n",
    "\n",
    "\n",
    "### But again we ask the question, was there a better word that eliminates the most uncertainty? \n",
    "\n",
    "We can find the answer by calculating the entropy of every word in our `word_pool` and seeing if there are higher entropy words -- words that that will eliminate even more uncertainty by generating the most evenly distributed feedback patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below this will calculate the entropy of every word in our `word_pool` and rank them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>brace</td>\n",
       "      <td>5.166653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brine</td>\n",
       "      <td>5.086516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>grace</td>\n",
       "      <td>5.068359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>grade</td>\n",
       "      <td>5.060797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dance</td>\n",
       "      <td>5.003984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arise</td>\n",
       "      <td>4.998408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>grand</td>\n",
       "      <td>4.975083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>brain</td>\n",
       "      <td>4.968449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>drake</td>\n",
       "      <td>4.955869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>drape</td>\n",
       "      <td>4.927298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word   entropy\n",
       "13  brace  5.166653\n",
       "12  brine  5.086516\n",
       "60  grace  5.068359\n",
       "61  grade  5.060797\n",
       "30  dance  5.003984\n",
       "1   arise  4.998408\n",
       "63  grand  4.975083\n",
       "16  brain  4.968449\n",
       "34  drake  4.955869\n",
       "31  drape  4.927298"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def calculate_entropy(guess, answer_list):\n",
    "    # Dictionary to hold counts of each feedback pattern\n",
    "    pattern_counts = {}\n",
    "\n",
    "    # Generate feedback patterns for each word in the answer list\n",
    "    for answer in answer_list:\n",
    "        pattern = get_feedback_pattern(guess, answer)\n",
    "        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1\n",
    "\n",
    "    # Calculate entropy based on the frequency of each pattern\n",
    "    entropy = 0\n",
    "    total_patterns = len(answer_list)\n",
    "    for count in pattern_counts.values():\n",
    "        probability = count / total_patterns\n",
    "        entropy -= probability * math.log(probability, 2)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "# data frame of words and their entropy\n",
    "df = pd.DataFrame(word_pool, columns=[\"word\"])\n",
    "\n",
    "# add entropy column\n",
    "df[\"entropy\"] = df[\"word\"].apply(lambda x: calculate_entropy(x, word_pool))\n",
    "\n",
    "df.sort_values(by=\"entropy\", ascending=False, inplace=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like `great` isn't even in the top 10, and `brace` is the best word we could have picked!\n",
    "\n",
    "Well, the proof is always in the pudding, so how many words remain if we use `brace` instead of `great` as our guess. The code below this will filter the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flame']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_guess_list(word_pool, 'brace', get_feedback_pattern('brace', 'flame'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Well how about that! `brace` filters the list down to just one word -- Looks like Bell Labs didn't employ many slouches. \n",
    "\n",
    "Of course this example used 70 words and the current version of WORDLE uses 2315 official answers and 14854 acceptable guesses. So the computational challenge scales up a little but, but not much --  I combined the functions found above in this notebook and created ALEX BOT, a pretty good wordle bot that isn't perfectly optimal but close to it and is fun to play around with, you can find it in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what's the best word to start a game of WORDLE with? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_pool = official_guesses\n",
    "answer_pool = official_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's find the top 20 5-letter words ranked by Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12822</th>\n",
       "      <td>TARSE</td>\n",
       "      <td>5.946725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13077</th>\n",
       "      <td>TIARE</td>\n",
       "      <td>5.931203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>SOARE</td>\n",
       "      <td>5.885960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>ROATE</td>\n",
       "      <td>5.882779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>RAISE</td>\n",
       "      <td>5.877910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10280</th>\n",
       "      <td>RAILE</td>\n",
       "      <td>5.865710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10435</th>\n",
       "      <td>REAST</td>\n",
       "      <td>5.865457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>SLATE</td>\n",
       "      <td>5.855775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>CRATE</td>\n",
       "      <td>5.834874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>SALET</td>\n",
       "      <td>5.834582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word   entropy\n",
       "12822  TARSE  5.946725\n",
       "13077  TIARE  5.931203\n",
       "12015  SOARE  5.885960\n",
       "10767  ROATE  5.882779\n",
       "10286  RAISE  5.877910\n",
       "10280  RAILE  5.865710\n",
       "10435  REAST  5.865457\n",
       "11829  SLATE  5.855775\n",
       "2637   CRATE  5.834874\n",
       "11095  SALET  5.834582"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data frame of entropy for every word in the guess_pool against the answer_pool\n",
    "entropy_df = pd.DataFrame(guess_pool, columns=[\"word\"])\n",
    "entropy_df[\"entropy\"] = entropy_df[\"word\"].apply(lambda x: calculate_entropy(x, answer_pool))\n",
    "entropy_df.sort_values(by=\"entropy\", ascending=False, inplace=True)\n",
    "entropy_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TARSE',\n",
       " 'TIARE',\n",
       " 'SOARE',\n",
       " 'ROATE',\n",
       " 'RAISE',\n",
       " 'RAILE',\n",
       " 'REAST',\n",
       " 'SLATE',\n",
       " 'CRATE',\n",
       " 'SALET',\n",
       " 'IRATE',\n",
       " 'TRACE',\n",
       " 'SATER',\n",
       " 'ARISE',\n",
       " 'ORATE',\n",
       " 'STARE',\n",
       " 'CARTE',\n",
       " 'RAINE',\n",
       " 'RANSE',\n",
       " 'CARET']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a list of the top 10 words with the highest entropy\n",
    "top_20_words = entropy_df[\"word\"].tolist()[:20]\n",
    "top_20_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use `AlexBot` to guess each of the 2315 words using each of the top 20 words as the starting guess. We'll take the mean of the number of guesses across all 2315 words and see which word has the lowest mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from alex_bot import simulate_all_wordle_games, load_word_list_from_py\n",
    "GUESS_LIST_PATH = \"word_lists/officialanswers.py\"\n",
    "ANSWER_LIST_PATH = \"word_lists/officialanswers.py\"\n",
    "guess_list = load_word_list_from_py(GUESS_LIST_PATH)\n",
    "answer_list = load_word_list_from_py(ANSWER_LIST_PATH)\n",
    "\n",
    "# simulate all wordle games for the top 10 words, make a data frame of the results\n",
    "results = pd.DataFrame()\n",
    "results[\"word\"] = top_20_words\n",
    "results['average_guesses'] = results[\"word\"].apply(lambda x: simulate_all_wordle_games(x, guess_list, answer_list))\n",
    "results.sort_values(by=\"average_guesses\", ascending=True, inplace=True)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>average_guesses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SALET</td>\n",
       "      <td>3.571058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TARSE</td>\n",
       "      <td>3.580994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SLATE</td>\n",
       "      <td>3.581425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TRACE</td>\n",
       "      <td>3.583585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>REAST</td>\n",
       "      <td>3.586609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CARTE</td>\n",
       "      <td>3.599136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CRATE</td>\n",
       "      <td>3.602160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CARET</td>\n",
       "      <td>3.613823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RANSE</td>\n",
       "      <td>3.615551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>STARE</td>\n",
       "      <td>3.619006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word  average_guesses\n",
       "9   SALET         3.571058\n",
       "0   TARSE         3.580994\n",
       "7   SLATE         3.581425\n",
       "11  TRACE         3.583585\n",
       "6   REAST         3.586609\n",
       "16  CARTE         3.599136\n",
       "8   CRATE         3.602160\n",
       "19  CARET         3.613823\n",
       "18  RANSE         3.615551\n",
       "15  STARE         3.619006"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, `SALET` is the winner(sorta)\n",
    "\n",
    "What's a `SALET`? The internet tells me it's some sort of helmet. The things you learn. \n",
    "\n",
    "The truth of the matter is that as of the date of the publication of this little notebook, there might be a better word, the NY Times now controls WORDLE and has been changing the word dataset. I believe that my `word_pools` are a mixture of pre and post nytimes WORDLE datasets and `TARSE` (going by the entropy numbers) is probably the best starting word, but it's impossible to validate without the current dataset, which is no longer publicly available. \n",
    "\n",
    "Still, there's only so many 5-letter words, so `SALET` is likely in the running for top words. So, put your SALET on and WORDLE on!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
